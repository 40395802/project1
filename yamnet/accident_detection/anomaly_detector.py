import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import sounddevice as sd
import librosa
import os
from sklearn.ensemble import IsolationForest
import joblib
from datetime import datetime
import matplotlib.pyplot as plt


class AccidentSoundAnomalyDetector:
    def __init__(self):
        # YAMNet ëª¨ë¸ ë¡œë“œ
        self.yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')
        
        # Isolation Forest ëª¨ë¸ (ì‚¬ê³  ê°ì§€ìš©)
        self.anomaly_detector = IsolationForest(
            contamination=0.1,  
            random_state=42,
            n_estimators=100
        )
        
        self.YAMNET_SAMPLE_RATE = 16000
        self.threshold = None  # ì„ê³„ê°’ (í•™ìŠµ ì‹œ ìë™ ì„¤ì •)
        
    def preprocess_audio(self, audio_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ YAMNet ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        audio_data, sr = librosa.load(audio_path, sr=self.YAMNET_SAMPLE_RATE)
        
        # YAMNet ì„ë² ë”© ì¶”ì¶œ
        _, embeddings, _ = self.yamnet_model(audio_data)
        
        # ì„ë² ë”©ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ë‹¨ì¼ ë²¡í„°ë¡œ ë³€í™˜
        embedding_mean = tf.reduce_mean(embeddings, axis=0)
        return embedding_mean.numpy()
    
    def train(self, accident_dir):
        """ì‚¬ê³ ìŒ ë°ì´í„°ë§Œìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ"""
        print("ì‚¬ê³ ìŒ ë°ì´í„° ë¡œë”© ì¤‘...")
        X = []  # ì„ë² ë”© íŠ¹ì§•
        
        # ì‚¬ê³ ìŒ íŒŒì¼ ì²˜ë¦¬
        for filename in os.listdir(accident_dir):
            if filename.endswith(('.wav', '.mp3')):
                audio_path = os.path.join(accident_dir, filename)
                embedding = self.preprocess_audio(audio_path)
                X.append(embedding)
        
        X = np.array(X)
        print(f"ì´ {len(X)}ê°œì˜ ì‚¬ê³ ìŒ ë°ì´í„°ë¡œ í•™ìŠµ")
        
        # Isolation Forest í•™ìŠµ
        print("ì´ìƒ ê°ì§€ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        self.anomaly_detector.fit(X)
        
        # ì„ê³„ê°’ ì„¤ì •ì„ ìœ„í•œ ì ìˆ˜ ê³„ì‚°
        scores = self.anomaly_detector.score_samples(X)
        self.threshold = np.percentile(scores, 5)  # í•˜ìœ„ 5% ê¸°ì¤€ìœ¼ë¡œ ì„ê³„ê°’ ì„¤ì •
        
        # ëª¨ë¸ ì €ì¥
        model_path = 'accident_anomaly_detector.joblib'
        joblib.dump({'model': self.anomaly_detector, 'threshold': self.threshold}, model_path)
        print(f"ëª¨ë¸ ì €ì¥ë¨: {model_path}")
    
    def load_model(self, model_path='accident_anomaly_detector.joblib'):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        saved_data = joblib.load(model_path)
        self.anomaly_detector = saved_data['model']
        self.threshold = saved_data['threshold']
    
    def predict_realtime(self, audio_data, input_sample_rate):
        """ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë°ì´í„°ì—ì„œ ì‚¬ê³ ìŒ ê°ì§€"""
        # ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬
        if audio_data.ndim > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        if input_sample_rate != self.YAMNET_SAMPLE_RATE:
            audio_data = librosa.resample(
                audio_data, 
                orig_sr=input_sample_rate, 
                target_sr=self.YAMNET_SAMPLE_RATE
            )
        
        # YAMNet ì„ë² ë”© ì¶”ì¶œ
        _, embeddings, _ = self.yamnet_model(audio_data)
        embedding_mean = tf.reduce_mean(embeddings, axis=0).numpy()
        
        # ì´ìƒ ê°ì§€ ì ìˆ˜ ê³„ì‚°
        anomaly_score = self.anomaly_detector.score_samples([embedding_mean])[0]
        
        # anomaly_score ê¸°ë³¸ê°’ ì„¤ì •
        if np.isnan(anomaly_score):
            anomaly_score = 0  # ê¸°ë³¸ê°’ ì„¤ì •

        # ì„ê³„ê°’ê³¼ ë¹„êµí•˜ì—¬ ì‚¬ê³ ìŒ ì—¬ë¶€ íŒë‹¨
        is_accident = anomaly_score < self.threshold
        
        # ì‹ ë¢°ë„ ê³„ì‚° 
        if self.threshold is not None:
            confidence = (self.threshold - anomaly_score) / np.abs(self.threshold)
        else:
            confidence = 0
        
        confidence = np.clip(confidence, 0, 1)
        
        # ìœ ì‚¬ë„ ê³„ì‚° (Isolation Forestì—ì„œ ì œê³µí•˜ëŠ” scoreë¥¼ í™œìš©)
        similarity = 1 - np.abs(anomaly_score / self.threshold)
        similarity = np.clip(similarity, 0, 1)
        
        # log-mel spectrogram, waveform 
        mel_spectrogram = self.compute_log_mel_spectrogram(audio_data, self.YAMNET_SAMPLE_RATE)
        waveform = audio_data  
        
        return is_accident, confidence, similarity, mel_spectrogram, waveform

    def compute_log_mel_spectrogram(self, audio_data, sample_rate):
        """log-mel spectrogram """
       
        D = librosa.stft(audio_data)
        
        mel_spectrogram = librosa.feature.melspectrogram(S=np.abs(D), sr=sample_rate, n_mels=128)
        
        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)
        
        return log_mel_spectrogram


def plot_spectrogram_and_waveform(mel_spectrogram, waveform, sample_rate):
    """ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # waveform
    ax1.plot(np.linspace(0, len(waveform) / sample_rate, len(waveform)), waveform)
    ax1.set_title("Waveform (Time Domain)")
    ax1.set_xlabel("Time (s)")
    ax1.set_ylabel("Amplitude")
    ax1.grid(True)
    
    # mel spectrogram
    img = ax2.imshow(mel_spectrogram, aspect='auto', origin='lower', cmap='inferno', extent=[0, mel_spectrogram.shape[-1] / sample_rate, 0, sample_rate / 2])
    ax2.set_title("Log-Mel Spectrogram")
    ax2.set_xlabel("Time (s)")
    ax2.set_ylabel("Frequency (Hz)")
    fig.colorbar(img, ax=ax2, format="%+2.0f dB")  
    ax2.grid(True)
    
    plt.tight_layout()  
    plt.show()


def start_monitoring(detector):
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
    SAMPLE_RATE = 48000
    DURATION = 1
    
    def audio_callback(indata, frames, time, status, detector):
        if status:
            print(status)
        
        audio_data = indata.copy()
        is_accident, confidence, similarity, mel_spectrogram, waveform = detector.predict_realtime(audio_data, SAMPLE_RATE)
        
        # í˜„ì¬ ì‹œê°„
        current_time = datetime.now().strftime("%H:%M:%S")
        
        # ìƒíƒœ ì¶œë ¥
        if is_accident:
            print(f"\rğŸš¨ [{current_time}] ì‚¬ê³ ìŒ ê°ì§€! (ì‹ ë¢°ë„: {confidence:.1%}, ìœ ì‚¬ë„: {similarity:.1%})", end="")
            
            # ì‚¬ê³ ìŒ ê°ì§€ ì‹œ ì‹œê°í™” ì¶œë ¥
            plot_spectrogram_and_waveform(mel_spectrogram, waveform, SAMPLE_RATE)
        else:
            print(f"\râœ… [{current_time}] ì •ìƒ (ì‹ ë¢°ë„: {confidence:.1%}, ìœ ì‚¬ë„: {similarity:.1%})", end="")
    
    with sd.InputStream(
        callback=lambda *args: audio_callback(*args, detector),
        channels=1,
        samplerate=SAMPLE_RATE,
        blocksize=int(SAMPLE_RATE * DURATION)
    ):
        print("ì‚¬ê³ ìŒ ê°ì§€ ëª¨ë‹ˆí„°ë§ ì‹œì‘... (Ctrl+Cë¡œ ì¢…ë£Œ)")
        try:
            while True:
                pass
        except KeyboardInterrupt:
            print("\nëª¨ë‹ˆí„°ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
